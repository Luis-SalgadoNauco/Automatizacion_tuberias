# AutomatizaciÃ³n de TuberÃ­as â€“ Apache Airflow

Repositorio de trabajo semanal para el aprendizaje prÃ¡ctico de **Apache Airflow**, enfocado en la creaciÃ³n, ejecuciÃ³n y documentaciÃ³n de DAGs para la automatizaciÃ³n de procesos de datos.

---

## DescripciÃ³n general de la semana

Durante esta semana se trabajarÃ¡ con Apache Airflow desde un entorno Ubuntu, partiendo desde una instalaciÃ³n limpia hasta la construcciÃ³n de workflows mÃ¡s completos.  
El enfoque serÃ¡ **prÃ¡ctico**, priorizando:

- Buenas prÃ¡cticas de estructura de proyectos
- Versionamiento correcto con Git y GitHub
- GeneraciÃ³n de evidencias de ejecuciÃ³n
- ComprensiÃ³n conceptual de DAGs y orquestaciÃ³n

Este repositorio se utiliza como **carpeta base para toda la semana**, evitando problemas de rutas y facilitando la trazabilidad del trabajo realizado.

---

## Objetivos de la semana

- Comprender quÃ© es Apache Airflow y para quÃ© se utiliza
- Crear DAGs funcionales desde cero
- Ejecutar DAGs manualmente y mediante scheduler
- Analizar logs y resultados de ejecuciÃ³n
- Documentar correctamente cada avance
- Mantener un repositorio ordenado y versionado

---

## DÃ­a 1 â€“ Primer DAG funcional en Apache Airflow

### Objetivo del dÃ­a

- Instalar Apache Airflow en Ubuntu
- Configurar el entorno virtual y `AIRFLOW_HOME`
- Crear y ejecutar un DAG bÃ¡sico
- Verificar la correcta ejecuciÃ³n desde la UI y la consola
- Generar evidencia del trabajo realizado

---

### Trabajo realizado

Durante el DÃ­a 1 se realizaron las siguientes actividades:

- CreaciÃ³n de un entorno virtual en Python
- InstalaciÃ³n de Apache Airflow
- InicializaciÃ³n y migraciÃ³n de la base de datos
- ConfiguraciÃ³n correcta del directorio `AIRFLOW_HOME`
- CreaciÃ³n del primer DAG llamado **`saludo_diario`**
- EjecuciÃ³n manual del DAG desde:
  - Interfaz Web (Graph View)
  - Consola mediante `airflow dags trigger`
  - Consola mediante `airflow dags test`
- GeneraciÃ³n de archivos de evidencia con la salida de ejecuciÃ³n

---

### DAG creado

**Nombre:** `saludo_diario`

**Tareas incluidas:**

1. `tarea_bash`  
   Imprime la fecha y hora de ejecuciÃ³n mediante un comando Bash.

2. `tarea_python`  
   Ejecuta una funciÃ³n Python que imprime un mensaje de saludo.

3. `tarea_esperar`  
   Simula un proceso de espera usando `sleep`.

Las tareas se ejecutan de forma **secuencial**, respetando la lÃ³gica definida en el DAG.

---

### Evidencia de ejecuciÃ³n â€“ DÃ­a 1

Las evidencias del DÃ­a 1 se encuentran en la carpeta `evidencia/` e incluyen:

- `ejecucion_saludo_diario.txt`  
  Salida de la ejecuciÃ³n manual del DAG.

- `detalle_ejecucion_dia1.png`  
  Captura detallada de la ejecuciÃ³n de tareas.

- `grafico_dia1.png`  
  VisualizaciÃ³n del DAG en **Graph View** desde la Web UI.

---

### VerificaciÃ³n â€“ DÃ­a 1

**Â¿QuÃ© es un DAG en Airflow?**  
Un DAG (Directed Acyclic Graph) es un grafo dirigido sin ciclos que define la estructura de un workflow, especificando tareas y sus dependencias.

**Â¿Para quÃ© sirve definir dependencias entre tareas?**  
Permite controlar el orden de ejecuciÃ³n, asegurar que ciertas tareas solo se ejecuten cuando otras hayan finalizado correctamente y evitar ejecuciones inconsistentes.

**Â¿CuÃ¡l es la diferencia entre ejecutar un DAG con `trigger` y con `test`?**  
- `trigger` ejecuta el DAG completo usando el scheduler.
- `test` ejecuta el DAG de forma local y secuencial, Ãºtil para pruebas y depuraciÃ³n.

**Â¿Por quÃ© es importante generar evidencia de ejecuciÃ³n?**  
Porque permite validar que el DAG funciona correctamente, facilita la auditorÃ­a del proceso y deja trazabilidad del trabajo realizado.

---

## DÃ­a 2 â€“ DAG con dependencias complejas

### Objetivo del dÃ­a

- Construir un DAG con mÃºltiples ramas y dependencias
- Ejecutar tareas en paralelo
- Comprender el flujo visual de un DAG complejo
- Ejecutar y validar el pipeline desde la consola
- Generar evidencia de ejecuciÃ³n

---

### Trabajo realizado

Durante el DÃ­a 2 se realizaron las siguientes actividades:

- CreaciÃ³n del DAG **`pipeline_ventas_complejo`**
- DefiniciÃ³n de tareas de:
  - PreparaciÃ³n de entorno
  - ExtracciÃ³n de datos
  - ValidaciÃ³n
  - TransformaciÃ³n
  - UniÃ³n de datos (join)
  - Carga final
  - Reporte de ejecuciÃ³n
- ImplementaciÃ³n de dependencias complejas con ejecuciÃ³n paralela
- EjecuciÃ³n del DAG mediante:
  - `airflow dags test`
  - `airflow dags trigger`
- VerificaciÃ³n del flujo en la Web UI (Graph View)

---

### Flujo del DAG

preparar_entorno â†’ [extraer_api, extraer_db]
extraer_api â†’ validar_api â†’ transformar_ventas â†˜
extraer_db â†’ validar_db â†’ transformar_productos â†˜ â†’ join_datos â†’ cargar_dw â†’ enviar_reporte

---

### Evidencia de ejecuciÃ³n â€“ DÃ­a 2

Las evidencias del DÃ­a 2 se encuentran en la carpeta `evidencia/`:

- `ejecucion_pipeline_ventas_complejo.txt`  
  Salida completa de la ejecuciÃ³n del pipeline.

- `detalle_ejecucion_dia2.png`  
  Captura del detalle de ejecuciÃ³n de las tareas.

- `grafico_dia2.png`  
  VisualizaciÃ³n del DAG con dependencias complejas en **Graph View**.

---

### VerificaciÃ³n â€“ DÃ­a 2

**Â¿CuÃ¡ndo usar PythonOperator en lugar de BashOperator?**  
Se utiliza PythonOperator cuando la lÃ³gica de la tarea requiere procesamiento, validaciones o transformaciones en Python.  
BashOperator es ideal para comandos del sistema, scripts o tareas simples de infraestructura.

**Â¿QuÃ© ventajas tiene definir dependencias explÃ­citas?**  
- Permite paralelismo controlado
- Mejora la claridad del flujo
- Facilita mantenimiento y escalabilidad
- Reduce errores por ejecuciÃ³n fuera de orden
---

## ğŸ“˜ DÃ­a 3 â€“ Operadores, Sensores y Operadores Personalizados

### Objetivo del dÃ­a

- Conocer los operadores mÃ¡s comunes de Apache Airflow
- Comprender el uso de sensores para esperar condiciones externas
- Crear y utilizar un operador personalizado
- Construir un DAG que combine sensores, operadores estÃ¡ndar y personalizados
- Verificar la ejecuciÃ³n correcta desde la UI y la consola
- Generar evidencia grÃ¡fica de la ejecuciÃ³n

---

### Trabajo realizado

Durante el DÃ­a 3 se realizaron las siguientes actividades:

- RevisiÃ³n de operadores comunes:
  - `BashOperator`
  - `PythonOperator`
- IntroducciÃ³n y uso de sensores:
  - `FileSensor` para esperar la llegada de archivos
- CreaciÃ³n de un operador personalizado para validaciÃ³n de datos
- ConstrucciÃ³n del DAG **`pipeline_con_sensores`**
- EjecuciÃ³n manual del DAG desde la consola
- Monitoreo del flujo de tareas desde la Web UI
- AnÃ¡lisis de estados y resoluciÃ³n de errores durante la ejecuciÃ³n
- GeneraciÃ³n de evidencia visual del DAG ejecutado correctamente

---

### DAG creado

**Nombre:** `pipeline_con_sensores`

**DescripciÃ³n:**  
Pipeline que espera la llegada de un archivo de ventas, valida su calidad, procesa la informaciÃ³n, genera un reporte y finalmente limpia los archivos temporales.

---

### Tareas incluidas

1. **`esperar_archivo_datos`** (`FileSensor`)  
   Espera la existencia del archivo `/tmp/datos_ventas.csv` antes de continuar el flujo.

2. **`validar_datos_ventas`** (Operador personalizado)  
   Lee el archivo CSV y valida la calidad de los datos segÃºn un umbral definido.

3. **`procesar_datos_ventas`** (`PythonOperator`)  
   Simula el procesamiento de los datos de ventas.

4. **`generar_reporte`** (`PythonOperator`)  
   Simula la generaciÃ³n de un reporte ejecutivo.

5. **`limpiar_archivos`** (`BashOperator`)  
   Elimina el archivo temporal utilizado en el proceso.

El flujo del DAG es **secuencial**:

## ğŸ“˜ DÃ­a 3 â€“ Operadores, Sensores y Operadores Personalizados

### Objetivo del dÃ­a

- Conocer los operadores mÃ¡s comunes de Apache Airflow
- Comprender el uso de sensores para esperar condiciones externas
- Crear y utilizar un operador personalizado
- Construir un DAG que combine sensores, operadores estÃ¡ndar y personalizados
- Verificar la ejecuciÃ³n correcta desde la UI y la consola
- Generar evidencia grÃ¡fica de la ejecuciÃ³n

---

### Trabajo realizado

Durante el DÃ­a 3 se realizaron las siguientes actividades:

- RevisiÃ³n de operadores comunes:
  - `BashOperator`
  - `PythonOperator`
- IntroducciÃ³n y uso de sensores:
  - `FileSensor` para esperar la llegada de archivos
- CreaciÃ³n de un operador personalizado para validaciÃ³n de datos
- ConstrucciÃ³n del DAG **`pipeline_con_sensores`**
- EjecuciÃ³n manual del DAG desde la consola
- Monitoreo del flujo de tareas desde la Web UI
- AnÃ¡lisis de estados y resoluciÃ³n de errores durante la ejecuciÃ³n
- GeneraciÃ³n de evidencia visual del DAG ejecutado correctamente

---

### DAG creado

**Nombre:** `pipeline_con_sensores`

**DescripciÃ³n:**  
Pipeline que espera la llegada de un archivo de ventas, valida su calidad, procesa la informaciÃ³n, genera un reporte y finalmente limpia los archivos temporales.

---

### Tareas incluidas

1. **`esperar_archivo_datos`** (`FileSensor`)  
   Espera la existencia del archivo `/tmp/datos_ventas.csv` antes de continuar el flujo.

2. **`validar_datos_ventas`** (Operador personalizado)  
   Lee el archivo CSV y valida la calidad de los datos segÃºn un umbral definido.

3. **`procesar_datos_ventas`** (`PythonOperator`)  
   Simula el procesamiento de los datos de ventas.

4. **`generar_reporte`** (`PythonOperator`)  
   Simula la generaciÃ³n de un reporte ejecutivo.

5. **`limpiar_archivos`** (`BashOperator`)  
   Elimina el archivo temporal utilizado en el proceso.

El flujo del DAG es **secuencial**:

esperar_archivo_datos â†’ validar_datos_ventas â†’ procesar_datos_ventas â†’ generar_reporte â†’ limpiar_archivos

### Evidencia de ejecuciÃ³n

Las evidencias del DÃ­a 3 se encuentran en la carpeta `evidencia/` e incluyen:

- `detalle_ejecucion_dia3.png` â€“ Detalle de ejecuciÃ³n de tareas
- `grafico_dia3.png` â€“ Vista grÃ¡fica del DAG ejecutado correctamente

---

### Aprendizajes clave

- Los sensores permiten sincronizar los DAGs con eventos externos.
- Un DAG no debe ejecutar tareas si no se cumplen las condiciones previas.
- Los operadores personalizados mejoran la reutilizaciÃ³n y limpieza del cÃ³digo.
- Airflow gestiona estados de tareas de forma independiente al resultado del scheduler.
- La Web UI es clave para depurar y entender la ejecuciÃ³n de pipelines.

---

### VerificaciÃ³n â€“ DÃ­a 3

**Â¿En quÃ© situaciones usarÃ­as un sensor en lugar de ejecutar tareas inmediatamente?**  
Se utiliza un sensor cuando una tarea depende de un evento externo, como la llegada de un archivo, la disponibilidad de un servicio o la finalizaciÃ³n de otro proceso. Esto evita fallos prematuros y permite que el flujo se ejecute solo cuando las condiciones son correctas.

**Â¿CuÃ¡les son las ventajas de crear operadores personalizados?**  
Permiten encapsular lÃ³gica reutilizable, mantener los DAGs mÃ¡s ordenados, estandarizar procesos, facilitar el mantenimiento y escalar soluciones de orquestaciÃ³n de forma mÃ¡s profesional.
---

## DÃ­as restantes (planificaciÃ³n)

- DÃ­a 4
- DÃ­a 5

---

## Estructura del proyecto

```text
airflow_curso/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ mi_primer_dag.py
â”‚   â”œâ”€â”€ pipeline_ventas_complejo.py
â”‚   â””â”€â”€ pipeline_con_sensores.py
â”œâ”€â”€ evidencia/
â”‚   â”œâ”€â”€ ejecucion_saludo_diario.txt
â”‚   â”œâ”€â”€ detalle_ejecucion_dia1.png
â”‚   â”œâ”€â”€ grafico_dia1.png
â”‚   â”œâ”€â”€ ejecucion_pipeline_ventas_complejo.txt
â”‚   â”œâ”€â”€ detalle_ejecucion_dia2.png
â”‚   â”œâ”€â”€ grafico_dia2.png
â”‚   â”œâ”€â”€ detalle_ejecucion_dia3.png
â”‚   â””â”€â”€ grafico_dia3.png
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
